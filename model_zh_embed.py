# model_zh_embed.py
import torch
import jieba
from transformers import BertTokenizer, BertModel
from sklearn.metrics.pairwise import cosine_similarity

MODEL_NAME = "bert-base-chinese"

# === è£ç½®é¸æ“‡ï¼šCUDA â†’ MPS â†’ CPU ===
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("âœ… ä½¿ç”¨ GPUï¼šCUDA")
elif torch.backends.mps.is_available() and torch.backends.mps.is_built():
    device = torch.device("mps")
    print("ðŸŽ ä½¿ç”¨ GPUï¼šApple MPS")
else:
    device = torch.device("cpu")
    print("ðŸ’» ä½¿ç”¨ CPUï¼šæœªåµæ¸¬åˆ°å¯ç”¨ GPU")

# === è¼‰å…¥æ¨¡åž‹èˆ‡ tokenizer ===
tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)
model = BertModel.from_pretrained(MODEL_NAME).to(device)
model.eval()

@torch.no_grad()
def embed_text(text: str) -> torch.Tensor:
    encoded = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=256,
        padding="max_length"
    ).to(device)

    output = model(**encoded)
    cls_vec = output.last_hidden_state[:, 0, :]  # å– [CLS] å‘é‡
    return cls_vec.cpu()  # å›žå‚³ CPU tensorï¼Œä¾› numpy ä½¿ç”¨

def extract_zh_keywords(text: str, top_n=5) -> list:
    words = list(set(jieba.lcut(text)))
    words = [w for w in words if len(w.strip()) >= 2]

    if not words:
        return []

    text_vec = embed_text(text).numpy()
    word_vecs = [embed_text(w).numpy()[0] for w in words]
    scores = cosine_similarity(text_vec, word_vecs)[0]

    ranked = sorted(zip(words, scores), key=lambda x: x[1], reverse=True)
    return ranked[:top_n]
